# OptiFormer-Lite

A Transformer-based hyperparameter optimization system. OptiFormer learns to suggest hyperparameter configurations by training on optimization trajectories generated by TPE (Tree-structured Parzen Estimator).

## Installation

```bash
pip install -r requirements.txt
pip install -e .
```

## Quick Start

### Run Smoke Test

The smoke test validates the entire pipeline end-to-end:

```bash
# Full smoke test
python -m smoke_test.run_all

# Quick smoke test (faster, less thorough)
python -m smoke_test.run_all --quick

# Skip real-world ML evaluation
python -m smoke_test.run_all --skip-realworld
```

### Run Unit Tests

```bash
pytest tests/ -v
```

## Project Structure

```
optiformer/
├── config/                 # Configuration files
│   ├── base.yaml          # Default configuration
│   ├── smoke_test.yaml    # Smoke test configuration
│   └── full_training.yaml # Full-scale training configuration
│
├── data/                  # Data handling
│   ├── generators/        # Synthetic function generation
│   ├── tokenizer/         # Tokenization (numerical & sequence)
│   └── datasets/          # PyTorch datasets
│
├── model/                 # Model architecture
│   ├── config.py          # Model configuration
│   ├── optiformer.py      # Main model (wraps LlamaForCausalLM)
│   └── generation.py      # Inference utilities
│
├── training/              # Training utilities
│   └── trainer.py         # Training loop
│
├── evaluation/            # Evaluation benchmarks
│   ├── synthetic_benchmarks/  # Sphere, Rastrigin, etc.
│   └── ml_benchmarks/        # MNIST, CIFAR-10, etc.
│
├── samplers/              # Optuna integration
│   └── optiformer_sampler.py  # Optuna-compatible sampler
│
├── smoke_test/            # Smoke test phases
│   ├── phase1_tokenizer.py
│   ├── phase2_data.py
│   ├── phase3_training.py
│   ├── phase4_synthetic.py
│   ├── phase5_realworld.py
│   └── run_all.py         # Master smoke test script
│
└── tests/                 # Unit tests
    └── test_tokenizer.py
```

## Architecture

### Tokenization

OptiFormer uses a custom tokenization scheme:
- **Numerical values**: Quantized to 1000 bins with optional log-scale
- **Categorical values**: Direct token mapping
- **Sequences**: `<BOS> <TARGET_REGRET> [<TRIAL_SEP> <PARAM> value ... <SCORE> score]* <EOS>`

### Model

- Decoder-only Transformer (based on LlamaForCausalLM)
- Three sizes: nano (~8M), small (~25M), base (~50M)
- RoPE positional embeddings
- Trained with next-token prediction

### Training Data

OptiFormer supports both synthetic and real-world training data:

**Synthetic Data:**
- **GP Prior Functions**: Smooth functions sampled from Gaussian Processes
- **Symbolic Functions**: Random mathematical expressions
- **Trajectories**: Generated by running TPE on synthetic functions

**Real-World Data (Optional):**
- **HPOBench**: Tabular benchmarks with precomputed results from FCNet training runs
- **OpenML**: Historical hyperparameter configurations from OpenML experiments
- **Custom Traces**: Collect trajectories from your own Optuna studies

## Usage

### Using OptiFormer as an Optuna Sampler

```python
import optuna
from optiformer.model import OptiFormer
from optiformer.data.tokenizer import SequenceTokenizer, ParameterSpec
from optiformer.samplers import OptiFormerSampler

# Define search space
param_specs = [
    ParameterSpec("learning_rate", 1e-5, 1e-1, log_scale=True),
    ParameterSpec("batch_size", 16, 256, is_integer=True),
    ParameterSpec("dropout", 0.0, 0.5),
]

# Load model and create sampler
model = OptiFormer.load("path/to/model.pt")
tokenizer = SequenceTokenizer(param_specs, num_bins=1000)
sampler = OptiFormerSampler(model, tokenizer)

# Use with Optuna
study = optuna.create_study(direction="minimize", sampler=sampler)
study.optimize(objective, n_trials=50)
```

### Training a Custom Model

```python
from optiformer.model import OptiFormer, OptiFormerConfig
from optiformer.training import OptiFormerTrainer, TrainingConfig
from optiformer.data.datasets import TrajectoryDataset

# Create model
config = OptiFormerConfig.small()
model = OptiFormer(config)

# Create datasets
train_dataset = TrajectoryDataset("train.jsonl", tokenizer)
val_dataset = TrajectoryDataset("val.jsonl", tokenizer)

# Train
trainer = OptiFormerTrainer(
    model=model,
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    config=TrainingConfig(max_steps=10000),
    output_dir="./outputs",
)
results = trainer.train()
```

### Training with Real-World Data

OptiFormer can be trained on a mix of synthetic and real-world data for better generalization:

```python
from pathlib import Path
from optiformer.data.datasets import (
    generate_real_world_data,
    create_mixed_dataloaders,
    RealWorldTraceCollector,
)

# Option 1: Generate data from HPOBench (requires hpobench installation)
real_data_paths = generate_real_world_data(
    output_dir=Path("./data/real_world"),
    use_hpobench=True,
    use_openml=False,
    hpobench_benchmarks=['fcnet_protein', 'fcnet_naval'],
    n_trajectories_per_benchmark=200,
)

# Option 2: Collect traces from your own Optuna studies
collector = RealWorldTraceCollector()
trajectories = collector.from_optuna_storage(
    storage_url="sqlite:///my_experiments.db",
    normalize_scores=True,
)
for traj in trajectories:
    collector.save_trajectory(traj, Path("./data/my_traces.jsonl"))

# Create mixed dataloaders (70% synthetic, 30% real-world by default)
train_loader, val_loader = create_mixed_dataloaders(
    synthetic_train_path=Path("./data/synthetic_train.jsonl"),
    synthetic_val_path=Path("./data/synthetic_val.jsonl"),
    real_world_paths=[Path("./data/real_world/hpobench_trajectories.jsonl")],
    tokenizer=tokenizer,
    batch_size=64,
    synthetic_weight=0.7,  # 70% synthetic, 30% real
)
```

**Installing Real-World Data Dependencies:**

```bash
# For HPOBench (may have complex dependencies)
pip install hpobench

# For OpenML traces
pip install openml>=0.14.0
```

## Success Criteria

The smoke test validates:

1. **Tokenizer**: Roundtrip error < 0.1%
2. **Training**: Loss reduction > 60%
3. **Synthetic Benchmarks**: Beat random search > 50% of the time
4. **ML Benchmarks**: Beat random search > 40% of the time

## License

MIT
